{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laboratorio 6 - Deep learning\n",
    "\n",
    "- Alejandra GuzmÃ¡n 20562\n",
    "- Jorge Caballeros 20009\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alegu\\AppData\\Local\\Temp\\ipykernel_5500\\3780130330.py:4: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  books_df = pd.read_csv('Books.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   User-ID                            Location   Age\n",
       " 0        1                  nyc, new york, usa   NaN\n",
       " 1        2           stockton, california, usa  18.0\n",
       " 2        3     moscow, yukon territory, russia   NaN\n",
       " 3        4           porto, v.n.gaia, portugal  17.0\n",
       " 4        5  farnborough, hants, united kingdom   NaN,\n",
       "          ISBN                                         Book-Title  \\\n",
       " 0  0195153448                                Classical Mythology   \n",
       " 1  0002005018                                       Clara Callan   \n",
       " 2  0060973129                               Decision in Normandy   \n",
       " 3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       " 4  0393045218                             The Mummies of Urumchi   \n",
       " \n",
       "             Book-Author Year-Of-Publication                   Publisher  \\\n",
       " 0    Mark P. O. Morford                2002     Oxford University Press   \n",
       " 1  Richard Bruce Wright                2001       HarperFlamingo Canada   \n",
       " 2          Carlo D'Este                1991             HarperPerennial   \n",
       " 3      Gina Bari Kolata                1999        Farrar Straus Giroux   \n",
       " 4       E. J. W. Barber                1999  W. W. Norton &amp; Company   \n",
       " \n",
       "                                          Image-URL-S  \\\n",
       " 0  http://images.amazon.com/images/P/0195153448.0...   \n",
       " 1  http://images.amazon.com/images/P/0002005018.0...   \n",
       " 2  http://images.amazon.com/images/P/0060973129.0...   \n",
       " 3  http://images.amazon.com/images/P/0374157065.0...   \n",
       " 4  http://images.amazon.com/images/P/0393045218.0...   \n",
       " \n",
       "                                          Image-URL-M  \\\n",
       " 0  http://images.amazon.com/images/P/0195153448.0...   \n",
       " 1  http://images.amazon.com/images/P/0002005018.0...   \n",
       " 2  http://images.amazon.com/images/P/0060973129.0...   \n",
       " 3  http://images.amazon.com/images/P/0374157065.0...   \n",
       " 4  http://images.amazon.com/images/P/0393045218.0...   \n",
       " \n",
       "                                          Image-URL-L  \n",
       " 0  http://images.amazon.com/images/P/0195153448.0...  \n",
       " 1  http://images.amazon.com/images/P/0002005018.0...  \n",
       " 2  http://images.amazon.com/images/P/0060973129.0...  \n",
       " 3  http://images.amazon.com/images/P/0374157065.0...  \n",
       " 4  http://images.amazon.com/images/P/0393045218.0...  ,\n",
       "    User-ID        ISBN  Book-Rating\n",
       " 0   276725  034545104X            0\n",
       " 1   276726  0155061224            5\n",
       " 2   276727  0446520802            0\n",
       " 3   276729  052165615X            3\n",
       " 4   276729  0521795028            6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "users_df = pd.read_csv('Users.csv')\n",
    "books_df = pd.read_csv('Books.csv')\n",
    "ratings_df = pd.read_csv('Ratings.csv')\n",
    "\n",
    "users_head = users_df.head()\n",
    "books_head = books_df.head()\n",
    "ratings_head = ratings_df.head()\n",
    "\n",
    "(users_head, books_head, ratings_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Users Summary':              User-ID                         Location            Age\n",
       "  count   278858.00000                           278858  168096.000000\n",
       "  unique           NaN                            57339            NaN\n",
       "  top              NaN  london, england, united kingdom            NaN\n",
       "  freq             NaN                             2506            NaN\n",
       "  mean    139429.50000                              NaN      34.751434\n",
       "  std      80499.51502                              NaN      14.428097\n",
       "  min          1.00000                              NaN       0.000000\n",
       "  25%      69715.25000                              NaN      24.000000\n",
       "  50%     139429.50000                              NaN      32.000000\n",
       "  75%     209143.75000                              NaN      44.000000\n",
       "  max     278858.00000                              NaN     244.000000,\n",
       "  'Users Missing Values': User-ID          0\n",
       "  Location         0\n",
       "  Age         110762\n",
       "  dtype: int64,\n",
       "  'Unique Locations': 57339,\n",
       "  'Age Distribution': count    168096.000000\n",
       "  mean         34.751434\n",
       "  std          14.428097\n",
       "  min           0.000000\n",
       "  25%          24.000000\n",
       "  50%          32.000000\n",
       "  75%          44.000000\n",
       "  max         244.000000\n",
       "  Name: Age, dtype: float64},\n",
       " {'Books Summary':               ISBN      Book-Title      Book-Author  Year-Of-Publication  \\\n",
       "  count       271360          271360           271359               271360   \n",
       "  unique      271360          242135           102023                  202   \n",
       "  top     0195153448  Selected Poems  Agatha Christie                 2002   \n",
       "  freq             1              27              632                13903   \n",
       "  \n",
       "          Publisher                                        Image-URL-S  \\\n",
       "  count      271358                                             271360   \n",
       "  unique      16807                                             271044   \n",
       "  top     Harlequin  http://images.amazon.com/images/P/185326119X.0...   \n",
       "  freq         7535                                                  2   \n",
       "  \n",
       "                                                Image-URL-M  \\\n",
       "  count                                              271360   \n",
       "  unique                                             271044   \n",
       "  top     http://images.amazon.com/images/P/185326119X.0...   \n",
       "  freq                                                    2   \n",
       "  \n",
       "                                                Image-URL-L  \n",
       "  count                                              271357  \n",
       "  unique                                             271041  \n",
       "  top     http://images.amazon.com/images/P/225307649X.0...  \n",
       "  freq                                                    2  ,\n",
       "  'Books Missing Values': ISBN                   0\n",
       "  Book-Title             0\n",
       "  Book-Author            1\n",
       "  Year-Of-Publication    0\n",
       "  Publisher              2\n",
       "  Image-URL-S            0\n",
       "  Image-URL-M            0\n",
       "  Image-URL-L            3\n",
       "  dtype: int64,\n",
       "  'Unique Authors': 102023,\n",
       "  'Publication Year Distribution': count     271360\n",
       "  unique       202\n",
       "  top         2002\n",
       "  freq       13903\n",
       "  Name: Year-Of-Publication, dtype: int64},\n",
       " {'Ratings Summary':             User-ID   Book-Rating\n",
       "  count  1.149780e+06  1.149780e+06\n",
       "  mean   1.403864e+05  2.866950e+00\n",
       "  std    8.056228e+04  3.854184e+00\n",
       "  min    2.000000e+00  0.000000e+00\n",
       "  25%    7.034500e+04  0.000000e+00\n",
       "  50%    1.410100e+05  0.000000e+00\n",
       "  75%    2.110280e+05  7.000000e+00\n",
       "  max    2.788540e+05  1.000000e+01,\n",
       "  'Ratings Missing Values': User-ID        0\n",
       "  ISBN           0\n",
       "  Book-Rating    0\n",
       "  dtype: int64,\n",
       "  'Rating Distribution': 0     716109\n",
       "  8     103736\n",
       "  10     78610\n",
       "  7      76457\n",
       "  9      67541\n",
       "  5      50974\n",
       "  6      36924\n",
       "  4       8904\n",
       "  3       5996\n",
       "  2       2759\n",
       "  1       1770\n",
       "  Name: Book-Rating, dtype: int64})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EDA - Users Dataset\n",
    "users_info = {\n",
    "    \"Users Summary\": users_df.describe(include='all'),\n",
    "    \"Users Missing Values\": users_df.isnull().sum(),\n",
    "    \"Unique Locations\": users_df['Location'].nunique(),\n",
    "    \"Age Distribution\": users_df['Age'].describe()\n",
    "}\n",
    "\n",
    "# EDA - Books Dataset\n",
    "books_info = {\n",
    "    \"Books Summary\": books_df.describe(include='all'),\n",
    "    \"Books Missing Values\": books_df.isnull().sum(),\n",
    "    \"Unique Authors\": books_df['Book-Author'].nunique(),\n",
    "    \"Publication Year Distribution\": books_df['Year-Of-Publication'].describe()\n",
    "}\n",
    "\n",
    "# EDA - Ratings Dataset\n",
    "ratings_info = {\n",
    "    \"Ratings Summary\": ratings_df.describe(),\n",
    "    \"Ratings Missing Values\": ratings_df.isnull().sum(),\n",
    "    \"Rating Distribution\": ratings_df['Book-Rating'].value_counts()\n",
    "}\n",
    "\n",
    "(users_info, books_info, ratings_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreProcesamiento de la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alegu\\AppData\\Local\\Temp\\ipykernel_5500\\2994511849.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  users_df_cleaned.Age.fillna(median_age, inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Users Cleaned':     User-ID                        Location   Age\n",
       " 1         2       stockton, california, usa  18.0\n",
       " 3         4       porto, v.n.gaia, portugal  17.0\n",
       " 5         6   santa monica, california, usa  61.0\n",
       " 9        10      albacete, wisconsin, spain  26.0\n",
       " 10       11  melbourne, victoria, australia  14.0,\n",
       " 'Users Missing Ages After Cleaning': 0,\n",
       " 'Books Cleaned':          ISBN                                         Book-Title  \\\n",
       " 0  0195153448                                Classical Mythology   \n",
       " 1  0002005018                                       Clara Callan   \n",
       " 2  0060973129                               Decision in Normandy   \n",
       " 3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       " 4  0393045218                             The Mummies of Urumchi   \n",
       " \n",
       "             Book-Author  Year-Of-Publication                   Publisher  \\\n",
       " 0    Mark P. O. Morford                 2002     Oxford University Press   \n",
       " 1  Richard Bruce Wright                 2001       HarperFlamingo Canada   \n",
       " 2          Carlo D'Este                 1991             HarperPerennial   \n",
       " 3      Gina Bari Kolata                 1999        Farrar Straus Giroux   \n",
       " 4       E. J. W. Barber                 1999  W. W. Norton &amp; Company   \n",
       " \n",
       "                                          Image-URL-S  \\\n",
       " 0  http://images.amazon.com/images/P/0195153448.0...   \n",
       " 1  http://images.amazon.com/images/P/0002005018.0...   \n",
       " 2  http://images.amazon.com/images/P/0060973129.0...   \n",
       " 3  http://images.amazon.com/images/P/0374157065.0...   \n",
       " 4  http://images.amazon.com/images/P/0393045218.0...   \n",
       " \n",
       "                                          Image-URL-M  \\\n",
       " 0  http://images.amazon.com/images/P/0195153448.0...   \n",
       " 1  http://images.amazon.com/images/P/0002005018.0...   \n",
       " 2  http://images.amazon.com/images/P/0060973129.0...   \n",
       " 3  http://images.amazon.com/images/P/0374157065.0...   \n",
       " 4  http://images.amazon.com/images/P/0393045218.0...   \n",
       " \n",
       "                                          Image-URL-L  \n",
       " 0  http://images.amazon.com/images/P/0195153448.0...  \n",
       " 1  http://images.amazon.com/images/P/0002005018.0...  \n",
       " 2  http://images.amazon.com/images/P/0060973129.0...  \n",
       " 3  http://images.amazon.com/images/P/0374157065.0...  \n",
       " 4  http://images.amazon.com/images/P/0393045218.0...  ,\n",
       " 'Books Missing Values After Cleaning': ISBN                   0\n",
       " Book-Title             0\n",
       " Book-Author            0\n",
       " Year-Of-Publication    0\n",
       " Publisher              0\n",
       " Image-URL-S            0\n",
       " Image-URL-M            0\n",
       " Image-URL-L            3\n",
       " dtype: int64}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Users Dataset Preprocessing\n",
    "# Remove unrealistic ages\n",
    "users_df_cleaned = users_df[(users_df.Age >= 5) & (users_df.Age <= 100)]\n",
    "\n",
    "# Fill missing ages with median value of the cleaned dataset\n",
    "median_age = users_df_cleaned.Age.median()\n",
    "users_df_cleaned.Age.fillna(median_age, inplace=True)\n",
    "\n",
    "# Books Dataset Preprocessing\n",
    "# Convert 'Year-Of-Publication' to numeric, coerce errors to NaN, and then convert NaNs to the median year\n",
    "books_df['Year-Of-Publication'] = pd.to_numeric(books_df['Year-Of-Publication'], errors='coerce')\n",
    "median_year = books_df['Year-Of-Publication'].median()\n",
    "books_df['Year-Of-Publication'].fillna(median_year, inplace=True)\n",
    "books_df['Year-Of-Publication'] = books_df['Year-Of-Publication'].astype(int)\n",
    "\n",
    "# Replace missing values in 'Book-Author' and 'Publisher' with 'Unknown'\n",
    "books_df['Book-Author'].fillna('Unknown', inplace=True)\n",
    "books_df['Publisher'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Ratings Dataset Preprocessing\n",
    "# For now, let's keep the ratings as they are, including the 0 ratings, and decide on the approach when building the recommendation system\n",
    "\n",
    "# Checking the results of the preprocessing\n",
    "preprocessed_data = {\n",
    "    \"Users Cleaned\": users_df_cleaned.head(),\n",
    "    \"Users Missing Ages After Cleaning\": users_df_cleaned['Age'].isnull().sum(),\n",
    "    \"Books Cleaned\": books_df.head(),\n",
    "    \"Books Missing Values After Cleaning\": books_df.isnull().sum()\n",
    "}\n",
    "\n",
    "preprocessed_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sistema de recomendacion basado en contenido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1                                          Clara Callan\n",
       "2                                  Decision in Normandy\n",
       "3     Flu: The Story of the Great Influenza Pandemic...\n",
       "4                                The Mummies of Urumchi\n",
       "5                                The Kitchen God's Wife\n",
       "6     What If?: The World's Foremost Military Histor...\n",
       "7                                       PLEADING GUILTY\n",
       "8     Under the Black Flag: The Romance and the Real...\n",
       "9               Where You'll Find Me: And Other Stories\n",
       "10                          Nights Below Station Street\n",
       "Name: Book-Title, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Load a smaller subset of the Books dataset for content-based recommendation system\n",
    "# Due to memory constraints, we will work with the first 1000 books\n",
    "books_df_subset = pd.read_csv('Books.csv', nrows=1000)\n",
    "\n",
    "# Use TF-IDF to convert the 'Book-Title' into a matrix of TF-IDF features\n",
    "tfidf_vectorizer_subset = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix_subset = tfidf_vectorizer_subset.fit_transform(books_df_subset['Book-Title'].fillna(''))\n",
    "\n",
    "# Compute the cosine similarity matrix based on the tfidf_matrix\n",
    "cosine_sim_matrix_subset = linear_kernel(tfidf_matrix_subset, tfidf_matrix_subset)\n",
    "\n",
    "# Function to get recommendations based on the cosine similarity score of book titles\n",
    "def get_content_based_recommendations_subset(title, cosine_sim=cosine_sim_matrix_subset):\n",
    "    # Get the index of the book that matches the title\n",
    "    idx = books_df_subset.index[books_df_subset['Book-Title'] == title].tolist()[0]\n",
    "\n",
    "    # Get the pairwise similarity scores of all books with that book\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the books based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the 10 most similar books\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # Get the book indices\n",
    "    book_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top 10 most similar books\n",
    "    return books_df_subset['Book-Title'].iloc[book_indices]\n",
    "\n",
    "# Testing the content-based recommendation function with a book title from the subset dataset\n",
    "test_book_title_subset = books_df_subset['Book-Title'].iloc[0]  # Get the first book title for testing\n",
    "content_based_recommendations_subset = get_content_based_recommendations_subset(test_book_title_subset)\n",
    "\n",
    "content_based_recommendations_subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['342677609X', '0805047379']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Load a smaller subset of the Ratings dataset for collaborative filtering recommendation system\n",
    "# We will work with the first 1000 ratings to fit within memory constraints\n",
    "ratings_df_subset = pd.read_csv('Ratings.csv', nrows=1000)\n",
    "\n",
    "# Create the user-item interaction matrix\n",
    "interaction_matrix = ratings_df_subset.pivot(index='User-ID', columns='ISBN', values='Book-Rating').fillna(0)\n",
    "\n",
    "# Convert the user-item interaction matrix to a sparse matrix format to save memory\n",
    "sparse_interaction_matrix = csr_matrix(interaction_matrix.values)\n",
    "\n",
    "# Initialize and fit the NearestNeighbors model (which is a memory-based collaborative filtering model)\n",
    "model_knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=20, n_jobs=-1)\n",
    "model_knn.fit(sparse_interaction_matrix)\n",
    "\n",
    "# Function to get book recommendations for a user using the collaborative filtering model\n",
    "def get_collaborative_filtering_recommendations(user_id, interaction_matrix, model_knn):\n",
    "    # Get the index of the user for which the recommendations are to be made\n",
    "    user_idx = interaction_matrix.index.tolist().index(user_id)\n",
    "\n",
    "    # Get the user interaction vector for the user\n",
    "    user_interaction_vector = interaction_matrix.values[user_idx, :].reshape(1, -1)\n",
    "\n",
    "    # Get the nearest neighbors (i.e., similar users)\n",
    "    distances, indices = model_knn.kneighbors(user_interaction_vector, n_neighbors=10)\n",
    "\n",
    "    # Initialize a list to store the ISBNs of the recommended books\n",
    "    recommended_books = []\n",
    "\n",
    "    # Loop through the indices of the neighbors\n",
    "    for idx in indices.flatten():\n",
    "        # Find books that the neighbor has rated 5 stars\n",
    "        high_rated_books = interaction_matrix.columns[interaction_matrix.iloc[idx] == 5].tolist()\n",
    "        recommended_books.extend(high_rated_books)\n",
    "\n",
    "    # Remove books that the user has already rated\n",
    "    rated_books = interaction_matrix.columns[interaction_matrix.iloc[user_idx] > 0].tolist()\n",
    "    recommended_books = list(set(recommended_books) - set(rated_books))\n",
    "\n",
    "    # Return the recommended books\n",
    "    return recommended_books\n",
    "\n",
    "# Testing the collaborative filtering recommendation function with a user from the subset dataset\n",
    "test_user_id_subset = ratings_df_subset['User-ID'].iloc[0]  # Get the first user ID for testing\n",
    "collaborative_filtering_recommendations_subset = get_collaborative_filtering_recommendations(\n",
    "    test_user_id_subset, interaction_matrix, model_knn\n",
    ")\n",
    "\n",
    "collaborative_filtering_recommendations_subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(271360, 54504)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine the 'Book-Author' and 'Publisher' columns into a single string for each book\n",
    "# We'll fill any NaN values with empty strings to ensure the vectorizer works correctly\n",
    "books_df['content'] = books_df['Book-Author'].fillna('') + ' ' + books_df['Publisher'].fillna('')\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Create the TF-IDF feature matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(books_df['content'])\n",
    "\n",
    "# Display the shape of the TF-IDF matrix to understand its size\n",
    "tfidf_matrix.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISBN</th>\n",
       "      <th>Book-Title</th>\n",
       "      <th>Book-Author</th>\n",
       "      <th>Publisher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599</th>\n",
       "      <td>0671898213</td>\n",
       "      <td>Baedeker London (Baedekers City Guides)</td>\n",
       "      <td>SONS</td>\n",
       "      <td>Macmillan General Reference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>0394826256</td>\n",
       "      <td>Walt Disney's Snow White and the Seven Dwarfs ...</td>\n",
       "      <td>Random House Editors</td>\n",
       "      <td>Random House Childrens Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>0439148790</td>\n",
       "      <td>Can You Hear a Shout in Space?: Questions and ...</td>\n",
       "      <td>Melvin Berger</td>\n",
       "      <td>Scholastic Reference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4675</th>\n",
       "      <td>0553502662</td>\n",
       "      <td>Disobedience</td>\n",
       "      <td>Jane Hamilton</td>\n",
       "      <td>Random House Audio Publishing Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2349</th>\n",
       "      <td>0517035820</td>\n",
       "      <td>Agatha Christie: Five Classic Murder Mysteries</td>\n",
       "      <td>Agatha Christie</td>\n",
       "      <td>Random House Value Publishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>0679431152</td>\n",
       "      <td>Disclosure</td>\n",
       "      <td>Michael Crichton</td>\n",
       "      <td>Random House Audio Publishing Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>0553473425</td>\n",
       "      <td>Bbc Presents: Middlemarch</td>\n",
       "      <td>George Eliot</td>\n",
       "      <td>Random House Audio Publishing Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2159</th>\n",
       "      <td>0671850245</td>\n",
       "      <td>The Complete Vampire Companion</td>\n",
       "      <td>Rosemary Ellen Guiley</td>\n",
       "      <td>Macmillan General Reference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3541</th>\n",
       "      <td>0679412093</td>\n",
       "      <td>The Trail Home: Essays</td>\n",
       "      <td>John Daniel</td>\n",
       "      <td>Random House Inc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4282</th>\n",
       "      <td>0679458182</td>\n",
       "      <td>Personal History</td>\n",
       "      <td>Katharine Graham</td>\n",
       "      <td>Random House Audio Publishing Group</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ISBN                                         Book-Title  \\\n",
       "1599  0671898213            Baedeker London (Baedekers City Guides)   \n",
       "1302  0394826256  Walt Disney's Snow White and the Seven Dwarfs ...   \n",
       "1307  0439148790  Can You Hear a Shout in Space?: Questions and ...   \n",
       "4675  0553502662                                       Disobedience   \n",
       "2349  0517035820     Agatha Christie: Five Classic Murder Mysteries   \n",
       "429   0679431152                                         Disclosure   \n",
       "1294  0553473425                          Bbc Presents: Middlemarch   \n",
       "2159  0671850245                     The Complete Vampire Companion   \n",
       "3541  0679412093                             The Trail Home: Essays   \n",
       "4282  0679458182                                   Personal History   \n",
       "\n",
       "                Book-Author                            Publisher  \n",
       "1599                   SONS          Macmillan General Reference  \n",
       "1302   Random House Editors         Random House Childrens Books  \n",
       "1307          Melvin Berger                 Scholastic Reference  \n",
       "4675          Jane Hamilton  Random House Audio Publishing Group  \n",
       "2349        Agatha Christie        Random House Value Publishing  \n",
       "429        Michael Crichton  Random House Audio Publishing Group  \n",
       "1294           George Eliot  Random House Audio Publishing Group  \n",
       "2159  Rosemary Ellen Guiley          Macmillan General Reference  \n",
       "3541            John Daniel                     Random House Inc  \n",
       "4282       Katharine Graham  Random House Audio Publishing Group  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Select a random subset of the books to avoid MemoryError\n",
    "# We'll take a subset of 5000 books for the demonstration\n",
    "subset_size = 5000\n",
    "books_subset = books_df.sample(n=subset_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Create the TF-IDF feature matrix for the subset\n",
    "tfidf_matrix_subset = tfidf_vectorizer.transform(books_subset['content'])\n",
    "\n",
    "# Compute the cosine similarity matrix from the TF-IDF matrix for the subset\n",
    "cosine_sim_matrix_subset = cosine_similarity(tfidf_matrix_subset, tfidf_matrix_subset)\n",
    "\n",
    "# Function to get book recommendations based on content similarity for the subset\n",
    "def get_content_based_recommendations_subset(book_idx, books_subset, cosine_sim_matrix, top_n=10):\n",
    "    # Get the pairwise similarity scores of all books with that book\n",
    "    sim_scores = list(enumerate(cosine_sim_matrix[book_idx]))\n",
    "\n",
    "    # Sort the books based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the top-n most similar books\n",
    "    sim_scores = sim_scores[1:top_n+1]\n",
    "\n",
    "    # Get the book indices\n",
    "    book_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top-n most similar books\n",
    "    return books_subset.iloc[book_indices]\n",
    "\n",
    "# Test the function with an example book index from our subset\n",
    "example_index = 0  # Using the first book in the subset for testing\n",
    "recommended_books_subset = get_content_based_recommendations_subset(example_index, books_subset, cosine_sim_matrix_subset)\n",
    "recommended_books_subset[['ISBN', 'Book-Title', 'Book-Author', 'Publisher']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red Basado en filtros colaborativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(         User-ID    ISBN  Book-Rating\n",
       " 686565     63068  108387            0\n",
       " 62456       4516   21407            0\n",
       " 1122931   101831   86132            7\n",
       " 636841     58113  202013            5\n",
       " 878589     80185   60802            0,\n",
       "          User-ID    ISBN  Book-Rating\n",
       " 178554     14710   66197            0\n",
       " 533905     48732  207194            8\n",
       " 1091374    98946   65482            0\n",
       " 1036247    93459  128975            0\n",
       " 309523     28004   47683            0,\n",
       "    User-ID    ISBN  Book-Rating\n",
       " 0   104433   57188            0\n",
       " 1   104434   29750            5\n",
       " 2   104435  107392            0\n",
       " 3   104436  127253            3\n",
       " 4   104436  127287            6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize label encoders\n",
    "user_encoder = LabelEncoder()\n",
    "book_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the encoders to the user ids and book isbns\n",
    "ratings_df['User-ID'] = user_encoder.fit_transform(ratings_df['User-ID'])\n",
    "ratings_df['ISBN'] = book_encoder.fit_transform(ratings_df['ISBN'])\n",
    "\n",
    "# Check the transformed data\n",
    "ratings_df_encoded_head = ratings_df.head()\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "train_data, test_data = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data.head(), test_data.head(), ratings_df_encoded_head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1, 50)        17027800    ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 1, 50)        5264150     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 50)           0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 50)           0           ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 100)          0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          12928       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 32)           4128        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 1)            33          ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 22,309,039\n",
      "Trainable params: 22,309,039\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Number of factors to use for embeddings\n",
    "n_factors = 50\n",
    "\n",
    "# Get the number of unique entities in users and books\n",
    "n_users = ratings_df['User-ID'].nunique()\n",
    "n_books = ratings_df['ISBN'].nunique()\n",
    "\n",
    "# Book embedding model\n",
    "book_input = Input(shape=(1,))\n",
    "book_embedding = Embedding(n_books, n_factors, input_length=1)(book_input)\n",
    "book_vec = Flatten()(book_embedding)\n",
    "\n",
    "# User embedding model\n",
    "user_input = Input(shape=(1,))\n",
    "user_embedding = Embedding(n_users, n_factors, input_length=1)(user_input)\n",
    "user_vec = Flatten()(user_embedding)\n",
    "\n",
    "# Concatenate features\n",
    "concat = Concatenate()([book_vec, user_vec])\n",
    "\n",
    "# Add fully-connected layers\n",
    "fc1 = Dense(128, activation='relu')(concat)\n",
    "fc2 = Dense(32, activation='relu')(fc1)\n",
    "output = Dense(1)(fc2)  # No activation, as this is a regression problem\n",
    "\n",
    "# Create model and compile it\n",
    "model = Model([user_input, book_input], output)\n",
    "model.compile(optimizer=Adam(0.001), loss='mean_squared_error')\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((919824,), (919824,), (919824,), 105283, 340556)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a new dataframe for the model that includes only the necessary columns\n",
    "model_data = ratings_df[['User-ID', 'ISBN', 'Book-Rating']].copy()\n",
    "\n",
    "# Initialize the label encoders\n",
    "user_le = LabelEncoder()\n",
    "book_le = LabelEncoder()\n",
    "\n",
    "# Fit the label encoders and transform the user ids and book ISBNs to encoded indices\n",
    "model_data['User-ID'] = user_le.fit_transform(model_data['User-ID'])\n",
    "model_data['ISBN'] = book_le.fit_transform(model_data['ISBN'])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, test_data = train_test_split(model_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare the input for the model\n",
    "train_user_data = train_data['User-ID'].values\n",
    "train_book_data = train_data['ISBN'].values\n",
    "train_ratings = train_data['Book-Rating'].values\n",
    "\n",
    "test_user_data = test_data['User-ID'].values\n",
    "test_book_data = test_data['ISBN'].values\n",
    "test_ratings = test_data['Book-Rating'].values\n",
    "\n",
    "# Number of unique users and books\n",
    "n_users = model_data['User-ID'].nunique()\n",
    "n_books = model_data['ISBN'].nunique()\n",
    "\n",
    "train_user_data.shape, train_book_data.shape, train_ratings.shape, n_users, n_books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 1, 50)        17027800    ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 1, 50)        5264150     ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 50)           0           ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 50)           0           ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 100)          0           ['flatten_2[0][0]',              \n",
      "                                                                  'flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 128)          12928       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           4128        ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1)            33          ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 22,309,039\n",
      "Trainable params: 22,309,039\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the neural network model for collaborative filtering\n",
    "# Number of factors to use for embeddings\n",
    "n_factors = 50\n",
    "\n",
    "# Book embedding model\n",
    "book_input = Input(shape=(1,))\n",
    "book_embedding = Embedding(n_books, n_factors, input_length=1)(book_input)\n",
    "book_vec = Flatten()(book_embedding)\n",
    "\n",
    "# User embedding model\n",
    "user_input = Input(shape=(1,))\n",
    "user_embedding = Embedding(n_users, n_factors, input_length=1)(user_input)\n",
    "user_vec = Flatten()(user_embedding)\n",
    "\n",
    "# Concatenate features\n",
    "concat = Concatenate()([book_vec, user_vec])\n",
    "\n",
    "# Add fully-connected layers\n",
    "fc1 = Dense(128, activation='relu')(concat)\n",
    "fc2 = Dense(32, activation='relu')(fc1)\n",
    "output = Dense(1)(fc2)  # No activation, as this is a regression problem\n",
    "\n",
    "# Create and compile the model\n",
    "model = Model([user_input, book_input], output)\n",
    "model.compile(optimizer=Adam(0.001), loss='mean_squared_error')\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1070/28745 [>.............................] - ETA: 1:53:51 - loss: 11.2315"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [train_user_data, train_book_data], \n",
    "    train_ratings, \n",
    "    validation_data=([test_user_data, test_book_data], test_ratings), \n",
    "    epochs=1, \n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
